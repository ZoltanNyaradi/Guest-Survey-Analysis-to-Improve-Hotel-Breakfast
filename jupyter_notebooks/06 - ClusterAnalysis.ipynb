{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cluster Analysis Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "* Fit and evaluate a cluster model to group similar data\n",
    "* Understand the profile for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "* General Dataset: outputs/datasets/collection/BreakfastSurvey.csv\n",
    "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs\n",
    "* Cluster Pipeline\n",
    "* Train Set\n",
    "* Most important features to define a cluster plot\n",
    "* Cluster Profile Description\n",
    "* Cluster Silhoulette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory.\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/collection/BreakfastSurvey.csv\")\n",
    "    .drop(\"breakfast\", axis=1)\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Pipeline with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Cluster Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"BoxCox\", vt.BoxCoxTransformer(['appearance', 'staff'])),\n",
    "\n",
    "        (\"yeo_johnson\", vt.YeoJohnsonTransformer(['price', 'taste', \"hotel\"])),\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "            method=\"spearman\", threshold=0.8, selection_method=\"variance\",    \n",
    "        )),\n",
    "\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
    "\n",
    "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
    "\n",
    "    ])\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
    "df_pca = pipeline_pca.fit_transform(df)\n",
    "\n",
    "print(df_pca.shape,'\\n', type(df_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA separately to the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "n_components = 5\n",
    "\n",
    "def pca_components_analysis(df_pca, n_components):\n",
    "    pca = PCA(n_components=n_components).fit(df_pca)\n",
    "    x_PCA = pca.transform(df_pca)\n",
    "\n",
    "    ComponentsList = [\"Component \" + str(number)\n",
    "                      for number in range(n_components)]\n",
    "    dfExplVarRatio = pd.DataFrame(\n",
    "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
    "        index=ComponentsList,\n",
    "        columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
    "    )\n",
    "\n",
    "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0, 110, 10))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components_analysis(df_pca=df_pca,n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"BoxCox\", vt.BoxCoxTransformer(['appearance', 'staff'])),\n",
    "\n",
    "        (\"yeo_johnson\", vt.YeoJohnsonTransformer(['price', 'taste', \"hotel\"])),\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "            method=\"spearman\", threshold=0.8, selection_method=\"variance\",    \n",
    "        )),\n",
    "\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        (\"PCA\", PCA(n_components=3, random_state=0)),\n",
    "\n",
    "        (\"model\", KMeans(n_clusters=6, random_state=0)),\n",
    "\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "PipelineCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method and Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(df)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,6))\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show()\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\" \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "n_cluster_start, n_cluster_stop = 2, 5\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.rcParams[\"font.family\"] = \"DejaVu Sans\" \n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline([\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
    "                                                              threshold=0.8, selection_method=\"variance\")),\n",
    "\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        (\"PCA\", PCA(n_components=3, random_state=0)),\n",
    "\n",
    "        (\"model\", KMeans(n_clusters=2, random_state=0)),\n",
    "\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "PipelineCluster()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick recap of our data for training cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cluster predictions to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a column `\"Clusters\"` (with the cluster pipeline predictions) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\" \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\n",
    "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
    "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
    "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
    "plt.xlabel(\"PCA Component 0\")\n",
    "plt.ylabel(\"PCA Component 1\")\n",
    "plt.title(\"PCA Components colored by Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the cluster predictions from this pipeline to use in the future. We will get back to that in a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables = X['Clusters']\n",
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a classifier, where the target is cluster predictions and features remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy X to a DataFrame df_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classifier pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def PipelineClf2ExplainClusters():\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"BoxCox\", vt.BoxCoxTransformer(['appearance', 'staff'])),\n",
    "\n",
    "        (\"yeo_johnson\", vt.YeoJohnsonTransformer(['price', 'taste', \"hotel\"])),\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "            method=\"spearman\", threshold=0.8, selection_method=\"variance\",    \n",
    "        )),\n",
    "        \n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        (\"feat_selection\", SelectFromModel(\n",
    "            GradientBoostingClassifier(random_state=0))),\n",
    "\n",
    "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
    "\n",
    "    ])\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineClf2ExplainClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate classifier performance on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test), zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the most important Features that define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_feat_eng_steps = 3\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
    ")].to_list()\n",
    "\n",
    "\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
    "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the best_features to use at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables = best_features\n",
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load function that plots a table with description for all Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DescriptionAllClusters(df, decimal_points=3):\n",
    "\n",
    "    DescriptionAllClusters = pd.DataFrame(\n",
    "        columns=df.drop(['Clusters'], axis=1).columns)\n",
    "   \n",
    "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
    "\n",
    "        EDA_ClusterSubset = df.query(\n",
    "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
    "        ClusterDescription = Clusters_IndividualDescription(\n",
    "            EDA_ClusterSubset, cluster, decimal_points)\n",
    "        DescriptionAllClusters = pd.concat(\n",
    "            [ClusterDescription, DescriptionAllClusters])\n",
    "\n",
    "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
    "    return DescriptionAllClusters\n",
    "\n",
    "\n",
    "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
    "\n",
    "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
    "\n",
    "    for col in EDA_Cluster.columns:\n",
    "\n",
    "        try:  \n",
    "\n",
    "            if EDA_Cluster[col].dtypes == 'object':\n",
    "\n",
    "                top_frequencies = EDA_Cluster.dropna(\n",
    "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
    "                Description = ''\n",
    "\n",
    "                for x in range(len(top_frequencies)):\n",
    "                    freq = top_frequencies.iloc[x]\n",
    "                    category = top_frequencies.index[x][0]\n",
    "                    CategoryPercentage = int(round(freq*100, 0))\n",
    "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
    "                    Description = Description + statement\n",
    "\n",
    "                ClustersDescription.at[0, col] = Description[:-2]\n",
    "\n",
    "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
    "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
    "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
    "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
    "                Description = f\"{Q1} -- {Q3}\"\n",
    "                ClustersDescription.at[0, col] = Description\n",
    "\n",
    "        except Exception as e:\n",
    "            ClustersDescription.at[0, col] = 'Not available'\n",
    "            print(\n",
    "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
    "\n",
    "    ClustersDescription['Cluster'] = str(cluster)\n",
    "\n",
    "    return ClustersDescription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def cluster_distribution_per_variable(df, target):\n",
    "    df_bar_plot = df.groupby(['Clusters', target]).size().reset_index(name='Count')\n",
    "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
    "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
    "\n",
    "    print(f\"Clusters distribution across {target} levels\")\n",
    "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
    "                 color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.show(renderer='jupyterlab')\n",
    "\n",
    "    df_relative = (df\n",
    "                   .groupby([\"Clusters\", target])\n",
    "                   .size()\n",
    "                   .unstack(fill_value=0)\n",
    "                   .apply(lambda x: 100 * x / x.sum(), axis=1)\n",
    "                   .stack()\n",
    "                   .reset_index(name='Relative Percentage (%)')\n",
    "                   .sort_values(by=['Clusters', target])\n",
    "                   )\n",
    "\n",
    "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
    "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
    "                  color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.show(renderer='jupyterlab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "\n",
    "df_cluster_profile[\"Clusters\"] =  df_cluster_profile[\"Clusters\"]\n",
    "\n",
    "print(df_cluster_profile.shape)\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_csv(\"outputs/datasets/collection/BreakfastSurvey.csv\").filter(['breakfast'])\n",
    "df_survey.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check breakfast leveles in the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_survey], axis=1), decimal_points=0)\n",
    "\n",
    "clusters_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_vs_breakfast =  df_survey.copy()\n",
    "df_cluster_vs_breakfast['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_breakfast, target='breakfast')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
